{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:21:43.347444Z",
     "start_time": "2021-07-04T18:21:37.626491Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:21:43.685532Z",
     "start_time": "2021-07-04T18:21:43.358422Z"
    }
   },
   "outputs": [],
   "source": [
    "#helper function\n",
    "import pandas as pd\n",
    "def show(df, no=10):  \n",
    "    return pd.DataFrame(df.take(no), columns=df.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:21:49.098652Z",
     "start_time": "2021-07-04T18:21:43.688102Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/Airports2.csv\", header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:21:49.171753Z",
     "start_time": "2021-07-04T18:21:49.102371Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:21:49.614880Z",
     "start_time": "2021-07-04T18:21:49.176173Z"
    }
   },
   "outputs": [],
   "source": [
    "show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save flights as one file per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:21:50.494883Z",
     "start_time": "2021-07-04T18:21:49.620262Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from pyspark.sql import functions as sf\n",
    "df = df.withColumn('month', sf.concat(year('Fly_date'), sf.lit('_'), month('Fly_date')))\n",
    "show(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:21:58.921480Z",
     "start_time": "2021-07-04T18:21:50.501823Z"
    }
   },
   "outputs": [],
   "source": [
    "show(df.groupBy(\"month\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:21:59.852389Z",
     "start_time": "2021-07-04T18:21:58.925630Z"
    }
   },
   "outputs": [],
   "source": [
    "#let's look what's inside one month\n",
    "df1 = df.where(\"month = '1999_10'\")\n",
    "show(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:22:08.750916Z",
     "start_time": "2021-07-04T18:21:59.857021Z"
    }
   },
   "outputs": [],
   "source": [
    "months = df.select(\"month\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:22:47.659327Z",
     "start_time": "2021-07-04T18:22:08.755369Z"
    }
   },
   "outputs": [],
   "source": [
    "#Process quite slow, so take only 2 of them\n",
    "\n",
    "for month in months[:2]:\n",
    "    df2 = df.where(f\"month = '{month[0]}'\")\n",
    "    #by adding coalesce(1) we save the dataframe to one file\n",
    "    df2.coalesce(1).write.mode(\"append\").option(\"header\", \"true\").csv(\"data/traffic_per_months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:22:47.964279Z",
     "start_time": "2021-07-04T18:22:47.661380Z"
    }
   },
   "outputs": [],
   "source": [
    "!cd data/traffic_per_months/ && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:22:48.526834Z",
     "start_time": "2021-07-04T18:22:47.966107Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's analyze what's in one file\n",
    "# Change file name\n",
    "part = spark.read.csv(\n",
    "    \"data/traffic_per_months/part-00000-6b669981-aa11-40fd-9074-972459682917-c000.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:22:49.914568Z",
     "start_time": "2021-07-04T18:22:48.529031Z"
    }
   },
   "outputs": [],
   "source": [
    "part.groupBy(\"month\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a stream of events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:28:19.509117Z",
     "start_time": "2021-07-04T18:28:19.425906Z"
    }
   },
   "outputs": [],
   "source": [
    "#Keep the same dataSchema\n",
    "dataSchema = part.schema\n",
    "part.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our Straming \n",
    "\n",
    "- *maxFilesPerTrigger* allows you to control how quickly Spark will read all of the files in the folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:22:49.999164Z",
     "start_time": "2021-07-04T18:22:49.928864Z"
    }
   },
   "outputs": [],
   "source": [
    "streaming = (\n",
    "    spark.readStream.schema(dataSchema)\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .csv(\"data/traffic_per_months/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:22:50.077313Z",
     "start_time": "2021-07-04T18:22:50.003836Z"
    }
   },
   "outputs": [],
   "source": [
    "airports_count = streaming.withColumn('Origin_city', F.concat('Origin_city', F.lit(', US'))) \\\n",
    "    .groupBy([\"Origin_city\", \"month\"]).count().orderBy(F.desc(\"count\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our transformation, we need to specify an output sink for the results. We're going to write to a memory sink which keeps the results in memory.\n",
    "\n",
    "We also need to define how Spark will output that data. We'll use the complete output mode (rewriting all of the keys along with their counts after every trigger).\n",
    "\n",
    "We won't include activityQuery.awaitTermination() because it is required only to prevent the driver process from terminating when the stream is active.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:22:50.404943Z",
     "start_time": "2021-07-04T18:22:50.080440Z"
    }
   },
   "outputs": [],
   "source": [
    "activityQuery = (\n",
    "    airports_count.writeStream.queryName(\"aiports\")\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# include this in production\n",
    "# activityQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:24:42.635378Z",
     "start_time": "2021-07-04T18:22:50.410456Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Follow http://localhost:4040/StreamingQuery\n",
    "import time\n",
    "\n",
    "for x in range(25):\n",
    "    df3 = spark.sql(\n",
    "        \"SELECT * FROM aiports where count > 2\"\n",
    "    )\n",
    "    if df3.count() > 0:\n",
    "        df3.show(10)\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if stream is active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:24:42.651011Z",
     "start_time": "2021-07-04T18:24:42.637743Z"
    }
   },
   "outputs": [],
   "source": [
    "spark.streams.active[0].isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:24:42.698305Z",
     "start_time": "2021-07-04T18:24:42.666380Z"
    }
   },
   "outputs": [],
   "source": [
    "activityQuery.status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we  want to turn off the stream we'll run activityQuery.stop() to reset the query for testing purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T18:24:42.895998Z",
     "start_time": "2021-07-04T18:24:42.723941Z"
    }
   },
   "outputs": [],
   "source": [
    "activityQuery.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T15:08:35.209448Z",
     "start_time": "2021-07-04T15:08:35.206413Z"
    }
   },
   "source": [
    "# Individual work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T15:22:15.505669Z",
     "start_time": "2021-07-04T15:22:15.494157Z"
    }
   },
   "source": [
    "1. Process at least 10 months worth of data (to have more files in the traffic_per_months folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-04T15:10:16.938516Z",
     "start_time": "2021-07-04T15:10:16.933774Z"
    }
   },
   "source": [
    "2. Using a new stream that processes 2 files at a time, implement the same functionality but for Destination_city"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
